{
 "cells": [
  {
<<<<<<< HEAD
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Description\n",
    "\"\"\"\n",
    "Last modified 08.02.2016\n",
=======
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Description\n",
    "\"\"\"\n",
    "Last modified 20.05.2016\n",
    "    Prepared for normalisations\n",
    "    \n",
    "Modified 11.02.2016\n",
    "    Added mongoDB support\n",
    "    pandas avr_table dumps to csv file in 'folder' directory \n",
    "\n",
    "08.02.2016\n",
>>>>>>> 4c12fd9fede7395ba51c0f31d7337ecd4c52bd05
    "Pandas functionality added, now can be saved to database\n",
    "\n",
    "New - sifting works correctly, \n",
    "        in dataD images which are sifted indicated as isgood=False, \n",
<<<<<<< HEAD
    "        fits now use parameter sigma = 'yerr' to calculate fit,\n",
    "        \n",
    "\n",
    "\"\"\""
=======
    "        fits now use parame\n",
    "        ter sigma = 'yerr' to calculate fit,\n",
    "        \n",
    "\n",
    "\"\"\";"
>>>>>>> 4c12fd9fede7395ba51c0f31d7337ecd4c52bd05
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# To restart remote kernels run code below before restarting this kernel\n",
    "rc1.close()"
   ]
  },
  {
   "cell_type": "raw",
<<<<<<< HEAD
   "metadata": {},
=======
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
>>>>>>> 4c12fd9fede7395ba51c0f31d7337ecd4c52bd05
   "source": [
    "%%px --targets 0 \n",
    "# to enter kernel qtconsole run this code\n",
    "%qtconsole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Imports and initialization\n",
    "%pylab inline\n",
    "\n",
    "import sys\n",
    "import os\n",
    "par_dir = os.path.split(os.getcwd())[0]\n",
    "if par_dir not in sys.path:\n",
    "    sys.path.append(par_dir)\n",
    "import inspect\n",
    "import pickle\n",
    "import imp\n",
    "import re\n",
    "import json\n",
    "from scipy.optimize import curve_fit\n",
    "import pandas as pd\n",
    "\n",
    "# from IPython.html import widgets\n",
    "# from IPython.display import display\n",
    "# from IPython.html.widgets import interact, interactive, fixed\n",
    "\n",
    "\n",
<<<<<<< HEAD
    "# import thulium_python_lib.usefull_functions as usfuncs\n",
=======
    "import thulium_python_lib.usefull_functions as usfuncs\n",
>>>>>>> 4c12fd9fede7395ba51c0f31d7337ecd4c52bd05
    "import thulium_python_lib.image_processing_new as impr\n",
    "\n",
    "import ipyparallel as ipp\n",
    "ipp.CompositeError.tb_limit = 1\n",
    "\n",
    "rc1 = ipp.Client()\n",
<<<<<<< HEAD
    "\n",
=======
>>>>>>> 4c12fd9fede7395ba51c0f31d7337ecd4c52bd05
    "lview = rc1.load_balanced_view()\n",
    "dview = rc1.direct_view()\n",
    "dview['par_dir'] = par_dir\n",
    "# with dview.sync_imports():\n",
    "#     import sys, os    \n",
    "%px import sys, os\n",
    "%px if par_dir not in sys.path: sys.path.append(par_dir)\n",
    "%px import thulium_python_lib.image_processing_new as impr\n",
    "%px import imp\n",
<<<<<<< HEAD
    "%px from ipyparallel import bind_kernel; bind_kernel()"
=======
    "# %px from ipyparallel import bind_kernel; bind_kernel()\n",
    "\n",
    "import datetime\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from bson.objectid import ObjectId\n",
    "\n",
    "# start mongoDB client (mongod server should be launched)\n",
    "# client = MongoClient('mongodb://localhost:27017/')\n",
    "client = MongoClient('mongodb://192.168.1.15:27017/')\n",
    "meas_database = client.measData.meas_data"
>>>>>>> 4c12fd9fede7395ba51c0f31d7337ecd4c52bd05
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to reload library on remote and local engine\n",
    "%px imp.reload(impr)\n",
    "imp.reload(impr)\n",
    "# imp.reload(usfuncs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "### And now:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
=======
    "### And now:\n",
>>>>>>> 4c12fd9fede7395ba51c0f31d7337ecd4c52bd05
    "#### Choose working directory and measurement folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# smth like 'D:\\!Data\\2015_08_20' for lab and like '/Users/artemgolovizin/Downloads/2015_08_20' for mac\n",
<<<<<<< HEAD
    "# os.chdir(r'\\\\BIGONE\\!Data\\2015_12_08')\n",
    "os.chdir(r'/Users/artemgolovizin/Documents/lab_data/2016_05_12')\n",
    "# print('Current directory', os.getcwd());\n",
    "\n",
=======
    "os.chdir(r'\\\\BIGONE\\!Data\\2016_05_08 T green next')\n",
    "# os.chdir(r'/Users/artemgolovizin/Downloads/2015_08_20/')\n",
    "# print('Current directory', os.getcwd());\n",
    "current_directory = os.path.split(os.getcwd())[-1]\n",
>>>>>>> 4c12fd9fede7395ba51c0f31d7337ecd4c52bd05
    "# Create folder 'Figures' for saving individual plot\n",
    "if not os.path.exists('Figures'):\n",
    "    os.makedirs('Figures')\n",
    "    print('Folder Figures has been created')\n",
<<<<<<< HEAD
    "    \n",
=======
>>>>>>> 4c12fd9fede7395ba51c0f31d7337ecd4c52bd05
    "working_directory = ''\n",
    "folder = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# specify working folder\n",
<<<<<<< HEAD
    "folder = '43 calib2'\n",
=======
    "folder = '01 T f=364.56 a=-5dBm/'\n",
>>>>>>> 4c12fd9fede7395ba51c0f31d7337ecd4c52bd05
    "working_directory = os.path.join(os.getcwd(),folder)\n",
    "# print('Working directory', working_directory)\n",
    "\n",
    "dirs = [x for x in os.listdir() if re.match('\\d',x)]\n",
<<<<<<< HEAD
    "meas_type, x_lbl, y_lbl, xaxis_calib = impr.get_x_calibration(folder, dirs)"
=======
    "meas_type, conf_params, x_lbl, y_lbl, xaxis_calib = impr.get_x_calibration(folder, dirs)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "meas_type, conf_params, x_lbl, y_lbl, xaxis_calib"
>>>>>>> 4c12fd9fede7395ba51c0f31d7337ecd4c52bd05
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data, rearange, average and calibrate\n",
    " Constract loader and averager. For available parameters see help('instance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
<<<<<<< HEAD
=======
    "code_folding": [],
>>>>>>> 4c12fd9fede7395ba51c0f31d7337ecd4c52bd05
    "collapsed": false
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "# attempts to use checkboxes for specify workflow\n",
    "from ipywidgets import Checkbox\n",
    "from IPython.display import display\n",
    "\n",
    "aaa = Checkbox(\n",
    "    description='Check me',\n",
    "    value=True,\n",
    ")\n",
    "display(aaa)"
=======
    "# create loader\n",
    "loader  = impr.Load_Image(dview)\n",
    "# downloading images\n",
    "all_data = loader(working_directory,lview)\n",
    "\n",
    "# rearranging to dictionary\n",
    "dataD = impr.rearrange_data(all_data)"
>>>>>>> 4c12fd9fede7395ba51c0f31d7337ecd4c52bd05
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
<<<<<<< HEAD
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create loader\n",
    "loader  = impr.Load_Image(dview)\n",
    "# downloading images\n",
    "all_data = loader(working_directory,lview)\n",
    "\n",
    "# rearranging to dictionary\n",
    "dataD = impr.rearrange_data(all_data)"
=======
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create averager\n",
    "averager = impr.Avr_Image(dview,do_sifting=True,conf_int=0.1)\n",
    "# averaging data and fitting\n",
    "avr_dataD = averager(dataD,lview)\n",
    "\n",
    "# construct new data dictionary without image and calibration atoms number, size and x-axis\n",
    "#navrD = impr.mod_avrData(avr_dataD, xaxis_calib, impr.N_atoms(width=0.5, delta = 5), impr.real_size)\n",
    "#gain_tmp=200 # 2016_05_04, 11, 12\n",
    "#gain_tmp=300 # 2016_05_05\n",
    "gain_tmp=400 # 2016_05_06, 07, 08\n",
    "navrD = impr.mod_avrData(avr_dataD, xaxis_calib, impr.N_atoms(gain=gain_tmp, exposure=200, power=6.3, width=1.85, delta = 5), impr.real_size)\n",
    "\n",
    "avr_table = impr.get_pandas_table2(navrD)\n",
    "# for plotting sifted image\n",
    "\n",
    "#imshow(imread('1 от частоты амплитудной модуляции аома верди (5) 3.9W/26ms/2_1.png'))\n",
    "#colorbar()"
>>>>>>> 4c12fd9fede7395ba51c0f31d7337ecd4c52bd05
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
<<<<<<< HEAD
=======
    "code_folding": [],
>>>>>>> 4c12fd9fede7395ba51c0f31d7337ecd4c52bd05
    "collapsed": false
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "dataD[0]"
=======
    "#plot(sum(avr_dataD[8][1].image,0))\n",
    "#print(avr_dataD[8][1].image.shape)"
>>>>>>> 4c12fd9fede7395ba51c0f31d7337ecd4c52bd05
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
<<<<<<< HEAD
    "code_folding": [],
=======
>>>>>>> 4c12fd9fede7395ba51c0f31d7337ecd4c52bd05
    "collapsed": false
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "# create averager\n",
    "averager = impr.Avr_Image(dview,do_sifting=True,conf_int=0.2)\n",
    "# averaging data\n",
    "avr_dataD = averager(dataD,lview)\n",
    "\n",
    "# construct new data dictionary without image and calibration atoms number, size and x-axis\n",
    "navrD = impr.mod_avrData(avr_dataD, xaxis_calib, impr.N_atoms(width=0.5, delta = 5), impr.real_size)\n",
    "\n",
    "avr_table = impr.get_pandas_table(navrD)\n",
    "# for plotting sifted image\n",
    "\n",
    "#imshow(imread('1 от частоты амплитудной модуляции аома верди (5) 3.9W/26ms/2_1.png'))\n",
    "#colorbar()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# convention table for filenames\n",
    "impr.meas_types"
=======
    "# convention table for filenames\n",
    "# impr.meas_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imshow(avr_dataD[9][1.0].image,vmax=0.02)\n",
    "#colorbar()"
>>>>>>> 4c12fd9fede7395ba51c0f31d7337ecd4c52bd05
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct data based on measurement type and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
<<<<<<< HEAD
    "code_folding": [],
    "collapsed": false
=======
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cloud_expansion_fixed_t0(x,T,r0,t0):\n",
    "    return usfuncs.cloud_expansion(x,T,r0,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "run_control": {
     "marked": false
    },
    "scrolled": false
>>>>>>> 4c12fd9fede7395ba51c0f31d7337ecd4c52bd05
   },
   "outputs": [],
   "source": [
    "# shot_typeN - for now only 1, if there will be calibration - 1 or more\n",
    "shot_typeN = 1\n",
    "\n",
    "# description to add to all_data.txt file\n",
    "description = dict()\n",
    "description['meas_type'] = meas_type\n",
    "description['x_label'] = x_lbl\n",
    "description['y_label'] = y_lbl\n",
    "\n",
<<<<<<< HEAD
    "#meas_type = 'T' # here one can specify type to get desired plots\n",
    "# meas_type = 'T'\n",
    "fit_func = None\n",
=======
    "# meas_type = 'T' # here one can specify type to get desired plots\n",
    "# meas_type = 'LT'\n",
    "fit_func = None\n",
    "fits_list = []\n",
>>>>>>> 4c12fd9fede7395ba51c0f31d7337ecd4c52bd05
    "fig1, ax1 = subplots()\n",
    "if meas_type == 'T':\n",
    "    # construct data with cloud expansion on both coordinates\n",
    "    # first set of data  - Temperature X\n",
    "    d1 = impr.get_avr_data(navrD, shot_typeN, 'fit1D_x',2)\n",
    "    d1['fmt']='ro'\n",
    "    d1['label']='fit1D_x'\n",
<<<<<<< HEAD
    "    #impr.drop_by_number(d1,5)\n",
    "    #impr.drop_by_x(d1,130,160)\n",
=======
    "    impr.drop_by_number(d1,4)\n",
    "#    impr.drop_by_x(d1,130,160)\n",
>>>>>>> 4c12fd9fede7395ba51c0f31d7337ecd4c52bd05
    "\n",
    "    # second set of data  - Temperature Y\n",
    "    d2 = impr.get_avr_data(navrD, shot_typeN,  'fit1D_y',2)\n",
    "    d2['fmt']='bo'\n",
    "    d2['label'] = 'fit1D_y'\n",
<<<<<<< HEAD
    "    #impr.drop_by_number(d2,5)\n",
    "    \n",
    "    # fit cloud expansion\n",
    "    fit_func = usfuncs.cloud_expansion0\n",
    "    popt_T, pcov_T = curve_fit(fit_func, d1['x'], d1['y'], p0=(d1['y'][0],20))\n",
    "    popt_T2, pcov_T2 = curve_fit(fit_func, d2['x'], d2['y'], p0=(d2['y'][0], 20))\n",
    "    print('Fit parameters X, Y:\\n' + usfuncs.construct_fit_description(fit_func, list(zip(popt_T,popt_T2))))\n",
    "    \n",
    "    # plot data and fits  ADD T VALUE ON PLOT\n",
    "    ax1.errorbar(**d2)\n",
    "    ax1.errorbar(**d1)\n",
    "    ax1.plot(linspace(min(d1['x']),max(d1['x']),100), fit_func(linspace(min(d1['x']),max(d1['x']),100),*popt_T),'k', label='Tx=%.2f'%(popt_T[0]))\n",
    "    ax1.plot(linspace(min(d2['x']),max(d2['x']),100), fit_func(linspace(min(d2['x']),max(d2['x']),100),*popt_T2),'k', label='Ty=%.2f'%(popt_T2[0]))\n",
=======
    "    impr.drop_by_number(d2,4)\n",
    "#     impr.drop_by_x(d2,14)\n",
    "    \n",
    "    # fit cloud expansion\n",
    "    fit_func = usfuncs.cloud_expansion0\n",
    "#     fit_func = cloud_expansion_fixed_t0\n",
    "    popt_T, pcov_T = curve_fit(fit_func, d1['x'], d1['y'], p0=(20,d1['y'][0]),bounds=(0,np.inf))\n",
    "    perr_T = np.sqrt(np.diag(pcov_T))\n",
    "    fits_list.append([fit_func.__name__,list(popt_T), list(perr_T)])\n",
    "    popt_T2, pcov_T2 = curve_fit(fit_func, d2['x'], d2['y'], p0=(20,d2['y'][0]),bounds=(0,np.inf))\n",
    "    perr_T2 = np.sqrt(np.diag(pcov_T2))\n",
    "    fits_list.append([fit_func.__name__,list(popt_T2), list(perr_T2)])\n",
    "    print('Fit parameters X, Y:\\n' + usfuncs.construct_fit_description(fit_func, list(zip(popt_T,popt_T2))))\n",
    "    \n",
    "    # plot data and fits \n",
    "    ax1.errorbar(**d2)\n",
    "    ax1.errorbar(**d1)\n",
    "    xx1 = linspace(min(d1['x']),max(d1['x']),100)\n",
    "    ax1.plot(xx1, fit_func(xx1,*popt_T),'r', label='Tx=%.2f$\\pm$%.1f'%(popt_T[0],perr_T[0]))\n",
    "    xx2=linspace(min(d2['x']),max(d2['x']),100)\n",
    "    ax1.plot(xx2, fit_func(xx2,*popt_T2), 'b', label='Ty=%.2f$\\pm$%.1f'%(popt_T2[0],perr_T2[0]))\n",
>>>>>>> 4c12fd9fede7395ba51c0f31d7337ecd4c52bd05
    "    ax1.set_xlim(min(d1['x'])-1, max(d1['x'])+1)\n",
    "    # add information about fits to description\n",
    "    description['fit1D_x'] =  popt_T\n",
    "    description['fit1D_y'] =  popt_T2\n",
    "    description['fit-function'] = fit_func.__name__\n",
    "else:\n",
<<<<<<< HEAD
    "    # construct data, param - what value to use\n",
    "    param = 'fit2D'\n",
    "    d1 = impr.get_avr_data(navrD, shot_typeN, param,0)\n",
    "    d1['fmt']='ro'\n",
    "    d1['label']=param\n",
    "    #impr.drop_by_number(d1,*range(20,len(d1['x'])))\n",
=======
    "    #construct data, param - what value to use\n",
    "    param = ['fit1D_x',0]\n",
    "    d1 = impr.get_avr_data(navrD, shot_typeN, *param)\n",
    "    d1['fmt']='ro'\n",
    "    d1['label']=param\n",
    "#     impr.drop_by_number(d1,0,1,2)\n",
>>>>>>> 4c12fd9fede7395ba51c0f31d7337ecd4c52bd05
    "    #impr.drop_by_x(d1,130,160)\n",
    "    \n",
    "    # fit_func - which function to use to fit data  here one can put conditions on meas_type to choose fit\n",
    "    popt_T = None\n",
<<<<<<< HEAD
    "#     fit_func = usfuncs.exp_decay\n",
    "#     popt_T, pcov_T = curve_fit(fit_func, d1['x'], d1['y'], p0=(d1['y'][0], 100,0))\n",
    "#     #popt_T, pcov_T = curve_fit(fit_func, d1['x'], d1['y'], p0=(-d1['y'][0], d1['x'][argmin(d1['y'])],0.1,d1['y'][0]))\n",
    "#     print('Fit parameters:\\n' + usfuncs.construct_fit_description(fit_func, popt_T))\n",
=======
    "#     fit_func = usfuncs.exp_decay_no_bg\n",
    "#     popt_T, pcov_T = curve_fit(fit_func, d1['x'], d1['y'], p0=(d1['y'][0], 100))\n",
    "    #popt_T, pcov_T = curve_fit(fit_func, d1['x'], d1['y'], p0=(-d1['y'][0], d1['x'][argmin(d1['y'])],0.1,d1['y'][0]))\n",
    "    #print('Fit parameters:\\n' + usfuncs.construct_fit_description(fit_func, popt_T))\n",
    "#     perr_T = np.sqrt(np.diag(pcov_T))\n",
>>>>>>> 4c12fd9fede7395ba51c0f31d7337ecd4c52bd05
    "    \n",
    "    # plot data and fits\n",
    "    ax1.errorbar(**d1)\n",
    "    if fit_func != None:\n",
    "        ax1.plot(linspace(min(d1['x']),max(d1['x']),100), fit_func(linspace(min(d1['x']),max(d1['x']),100),*popt_T),'k', label='fit X')\n",
<<<<<<< HEAD
    "    if fit_func != None:\n",
    "        fit_label = fit_func.__name__ + ' fit:\\n' + usfuncs.construct_fit_description(fit_func, popt_T)\n",
    "        ax1.text(0.01,0.01,fit_label,transform=ax1.transAxes)\n",
    "        description['fit'] =  popt_T\n",
    "        description['fit-function'] = fit_func.__name__\n",
=======
    "        fit_label = fit_func.__name__ + ' fit:\\n' + usfuncs.construct_fit_description(fit_func, list(zip(popt_T,perr_T)),sep='$\\pm$')\n",
    "        ax1.text(0.01,0.01,fit_label,transform=ax1.transAxes)\n",
    "        description['fit'] =  popt_T\n",
    "        description['fit-function'] = fit_func.__name__\n",
    "        fits_list.append([fit_func.__name__,list(popt_T), list(perr_T)])\n",
>>>>>>> 4c12fd9fede7395ba51c0f31d7337ecd4c52bd05
    "    \n",
    "\n",
    "ax1.set_xlabel(x_lbl)\n",
    "ax1.set_ylabel(y_lbl)\n",
    "ax1.set_title(folder.rstrip(r'\\/ '))\n",
    "ax1.set_ylim(bottom=0)\n",
<<<<<<< HEAD
    "ax1.legend(loc='best')"
=======
    "ax1.legend(loc=4)\n",
    "\n",
    "print('Number of atoms', avr_table.T[0].fit1D_x.N, '+-', avr_table.T[0].fit1D_x_std.N)"
>>>>>>> 4c12fd9fede7395ba51c0f31d7337ecd4c52bd05
   ]
  },
  {
   "cell_type": "raw",
<<<<<<< HEAD
   "metadata": {},
   "source": [
    "d1 = dict()\n",
    "param = ['fit1D_x','x0']\n",
=======
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# When change to pandas data\n",
    "d1 = dict()\n",
    "param = ['fit1D_x','N']\n",
>>>>>>> 4c12fd9fede7395ba51c0f31d7337ecd4c52bd05
    "d1['x']=avr_table['folder']\n",
    "d1['y'] = avr_table[tuple(param)]\n",
    "param_std = list(copy(param))\n",
    "param_std[0]+='_std'\n",
    "try:\n",
    "    d1['yerr'] = avr_table[tuple(param_std)]\n",
    "except:\n",
    "    d1['yerr'] = None\n",
<<<<<<< HEAD
    "d1['label']='_'.join(param)"
=======
    "d1['label']='_'.join(param)\n",
    "errorbar(**d1)\n",
    "legend(loc='best')"
>>>>>>> 4c12fd9fede7395ba51c0f31d7337ecd4c52bd05
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
<<<<<<< HEAD
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
=======
    "collapsed": false,
    "scrolled": true
>>>>>>> 4c12fd9fede7395ba51c0f31d7337ecd4c52bd05
   },
   "outputs": [],
   "source": [
    "# for each picture\n",
<<<<<<< HEAD
    "shot_typeN=2\n",
    "xs, ys, cs = impr.constract_data_scatter(dataD, shot_typeN, 'fit1D_y',0)\n",
    "plt.figure(figsize=(13,4))\n",
=======
    "shot_typeN = 1\n",
    "xs, ys, cs = impr.constract_data_scatter(dataD, shot_typeN, 'fit1D_y',0)\n",
    "# plt.figure(figsize=(13,4))\n",
>>>>>>> 4c12fd9fede7395ba51c0f31d7337ecd4c52bd05
    "scatter(xs,ys,c=cs,linewidths=0)\n",
    "title(\"Individual data, blue - good, red - bad\")\n",
    "ylim(bottom=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# show 2D image\n",
    "# imshow(avr_dataD[0.2][1].image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "if folder != '':\n",
    "    fig1.savefig(os.path.join('Figures',folder.rstrip(r'\\/ ')+'.png'))\n",
=======
    "\n",
    "if folder != '':\n",
    "    fig1.savefig(os.path.join('Figures',folder.rstrip(r'\\/ ')+'.png'))\n",
    "    # save pandas table to csv file\n",
    "    avr_table.to_csv(os.path.join(folder,'avr_table.csv'))\n",
>>>>>>> 4c12fd9fede7395ba51c0f31d7337ecd4c52bd05
    "    try:\n",
    "        with open('all_data.txt', 'rb') as handle:\n",
    "            res_dict = pickle.loads(handle.read())\n",
    "    except FileNotFoundError:\n",
    "        res_dict = {}\n",
    "    except EOFError:\n",
    "        res_dict = {}\n",
    "    res_dict[folder.rstrip(r'\\/ ')]={'description':description,'data':navrD}\n",
    "    with open('all_data.txt', 'wb') as handle:\n",
    "        pickle.dump(res_dict, handle)\n",
    "    with open('all_data.json', 'w') as outfile:\n",
    "        json.dump(res_dict, outfile, cls=impr.JsonCustomEncoder, indent=4)    \n",
    "    print('Figure and data saved!!!')\n",
    "    \n",
    "# to later read json file run following two lines\n",
    "#with open('all_data.json', 'r') as infile:\n",
<<<<<<< HEAD
    "#        ddata = json.load(infile)"
=======
    "#        ddata = json.load(infile)\n",
    "\n",
    "#save to mongo db\n",
    "# get pickle string of avr_table \n",
    "ss = 'temp'\n",
    "avr_table.to_pickle(ss)\n",
    "with open(ss,'rb') as fl:\n",
    "    line = fl.read()\n",
    "os.remove(ss)\n",
    "# prepear dictionary to load to mongoDB\n",
    "data_to_db = {\n",
    "             'date_meas':datetime.datetime.strptime(current_directory[:10],'%Y_%m_%d'),\n",
    "             'date_mod':datetime.datetime.now(),\n",
    "             'folder':folder,\n",
    "             'meas_type':meas_type,\n",
    "             'labels':[x_lbl, y_lbl],\n",
    "              'conf_params':conf_params,\n",
    "              'fits': fits_list,\n",
    "              'avr_table_pickle':line\n",
    "             }\n",
    "# try to find entery with the same 'date_meas' and 'folder' and either update or create entery\n",
    "res = meas_database.find_one({'date_meas':datetime.datetime.strptime(current_directory[:10],'%Y_%m_%d'),\n",
    "                 'folder':folder})\n",
    "if res:\n",
    "    print('Entery for folder \"%s\" updated' % folder)\n",
    "    meas_database.update_one({'_id':res['_id']},{'$set':data_to_db})\n",
    "else:\n",
    "    print('Entery for folder \"%s\" created' % folder)\n",
    "    meas_database.insert_one(data_to_db)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# part for getting data from mongoDB\n",
    "coursor = meas_database.find()\n",
    "tables = []\n",
    "for item in coursor:\n",
    "    for key in item:\n",
    "        if(key == 'avr_table_pickle'):\n",
    "            tables.append(pickle.loads(item[key]))\n",
    "            print(key,item[key][-20:])\n",
    "        else:\n",
    "            print(key,item[key])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# to delite some entery\n",
    "meas_database.delete_one({'folder':\"01 T preramp_a/\"})"
>>>>>>> 4c12fd9fede7395ba51c0f31d7337ecd4c52bd05
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "# to check how good is fit\n",
    "perr = np.sqrt(np.diag(pcov_T))\n",
    "print(perr)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# some attempts to use pandas for individual images data\n",
    "def construct_DataFrame(dictionary):\n",
    "    indexes = []\n",
    "    data = []\n",
    "    tr_table = {'fit1D':['N','x0','sigma','bgnd'], \n",
    "            'fit2D':['N','y0','x0','sigma_y','sigma_x','bgnd'],\n",
    "            'center_pos':['x','y']}\n",
    "    for key,value in dictionary.items():\n",
    "        \n",
    "        flag = False\n",
    "        if key == 'image':\n",
    "            continue\n",
    "        for tkey in tr_table:\n",
    "            if key.startswith(tkey):\n",
    "                for i, name in enumerate(tr_table[tkey]):\n",
    "                    indexes.append((key,name))\n",
    "                    data.append(value[i])\n",
    "                flag = True\n",
    "                continue\n",
    "        if not flag:    \n",
    "            indexes.append((key,''))\n",
    "            data.append(value)\n",
    "    res = pd.DataFrame(data, index=pd.MultiIndex.from_tuples(indexes)).T\n",
    "    return res\n",
    "data_pd = []\n",
    "for item in all_data:\n",
    "    data_pd.append(construct_DataFrame(item.__dict__))\n",
    "data_pd = pd.concat(data_pd,ignore_index=True).convert_objects(convert_numeric=True)\n",
    "for folder in data_pd.folderN.unique():\n",
    "    for tp in data_pd.shot_typeN.unique():\n",
    "        med = data_pd[(data_pd.folderN==folder)&  (data_pd.shot_typeN==tp)].total.median()\n",
    "#         print(abs(data_pd[(data_pd.folderN==folder)&  (data_pd.shot_typeN==tp)].total - med)/med < 0.1)\n",
    "        data_pd.loc[(data_pd.folderN==folder)&  (data_pd.shot_typeN==tp),'isgood'] = \n",
    "            abs(data_pd[(data_pd.folderN==folder)&  (data_pd.shot_typeN==tp)].total - med)/med < 0.1\n",
    "\n",
    "if len(data_pd.shot_typeN.unique()) == 2:\n",
    "    data_pd.loc[data_pd.shot_typeN==1,'total_small'] = np.divide(data_pd.loc[data_pd.shot_typeN==1,'total_small'],\n",
    "                                                                 data_pd.loc[data_pd.shot_typeN==2,'total_small'])\n",
    "    data_pd.loc[data_pd.shot_typeN==1,'isgood'] = np.multiply(data_pd.loc[data_pd.shot_typeN==1,'isgood'],\n",
    "                                                              data_pd.loc[data_pd.shot_typeN==2,'isgood'])"
=======
    "from random import shuffle\n",
    "scans = arange(0,3.1,0.1)\n",
    "shuffle(scans)\n",
    "' '.join((str(i) for i in scans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_value(obj, attribute, index):\n",
    "    \"\"\"retruns obj.attibute[index] or obj.attribute if index is not defined\"\"\"\n",
    "    if index != None:\n",
    "        return getattr(obj,attribute)[index]\n",
    "    else:\n",
    "        return getattr(obj,attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalise_individual_image(dictionary, signal_shot, calibration_shot, attribute, index=None, do_fit2D = False):\n",
    "    \"\"\"normalize each image using attribute[index] value - usually 'total' or 'x_data_fit[0]'\n",
    "        returns constracted dictionary (like what returns 'load_data()' function\"\"\"\n",
    "    norm_data = dict()\n",
    "    for folderN, f_dict in dictionary.items():\n",
    "        calibrated_images = []\n",
    "        for s_elem in f_dict[signal_shot]:\n",
    "            c_elems = [c_elem for c_elem in f_dict[calibration_shot] if c_elem.shotN == s_elem.shotN]\n",
    "            if c_elems == []:\n",
    "                print('s_elem.image_url has no calibration image')\n",
    "                continue\n",
    "            new_image = impr.Image_Basics(s_elem.image / get_value(c_elems[0],attribute,index))\n",
    "            new_image.fit1D_x = new_image.fit_gaussian1D(0)\n",
    "            new_image.fit1D_y = new_image.fit_gaussian1D(1)\n",
    "            calibrated_images = append(calibrated_images, new_image)\n",
    "        if calibrated_images != []:\n",
    "            norm_data[folderN] = dict()\n",
    "            norm_data[folderN][signal_shot] = calibrated_images\n",
    "    print('Normalization is complited')\n",
    "    return norm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bb = normalise_individual_image(dataD, 1, 2, 'fit1D_x', index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shot_typeN = 1\n",
    "xs, ys, cs = impr.constract_data_scatter(bb, shot_typeN, 'fit1D_y',0)\n",
    "# plt.figure(figsize=(13,4))\n",
    "scatter(xs,ys,c=cs,linewidths=0)\n",
    "scatter(xs2,ys2/5.3,c='r',linewidths=0)\n",
    "title(\"Individual data, blue - good, red - bad\")\n",
    "ylim(bottom=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shot_typeN = 1\n",
    "xs2, ys2, cs2 = impr.constract_data_scatter(dataD, shot_typeN, 'fit1D_x',0)\n",
    "xs3, ys3, cs3 = impr.constract_data_scatter(dataD, 2, 'fit1D_x',0)\n",
    "# plt.figure(figsize=(13,4))\n",
    "scatter(xs2,ys2/5,c='r',linewidths=0)\n",
    "title(\"Individual data, blue - good, red - bad\")\n",
    "ylim(bottom=0)"
>>>>>>> 4c12fd9fede7395ba51c0f31d7337ecd4c52bd05
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
